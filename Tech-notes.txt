# Technical Documentation: MARBET Event Assistant

## Overview

The MARBET Event Assistant is a sophisticated AI-powered conversational system designed to provide accurate information about a luxury travel experience (specifically a cruise from Canada to the USA in October 2024). The system leverages retrieval-augmented generation (RAG) techniques to provide document-grounded responses to user queries.

This documentation explains the technical implementation, architecture, and key components of the codebase.

## Architecture

The system consists of four main components:

1. **Document Processing**: Custom semantic text splitting, document loading, and preprocessing
2. **Vector Storage**: Embedding generation and efficient vector search
3. **Language Model Integration**: LLM-based conversational retrieval
4. **User Interface**: Both command-line and web-based interfaces

## Key Technologies

- **LangChain**: Framework for language model applications
- **Ollama**: For accessing local language models
- **FAISS**: Facebook AI Similarity Search for efficient vector storage
- **Gradio**: Web UI framework for ML applications
- **NLTK**: Natural Language Toolkit for sentence tokenization
- **Matplotlib**: For data visualization

## Component Details

### 1. Custom Semantic Text Splitter

One of the most innovative aspects of this implementation is the `SemanticTextSplitter` class that improves upon standard text chunking methods:

```python
class SemanticTextSplitter(TextSplitter):
    """A text splitter that uses semantic boundaries like paragraphs, sections, and sentences."""
```

**Key features**:

- **Boundary Respect**: Preserves paragraph and sentence boundaries
- **Hierarchical Splitting**: First splits by paragraphs, then falls back to sentences if needed
- **Adaptive Chunking**: Handles varying content lengths appropriately
- **Overflow Handling**: Employs fallback strategies for extremely long sentences

**Why this matters**: Traditional character-based splitting can break semantic units, reducing retrieval relevance. By respecting natural language boundaries, this approach creates more meaningful chunks that maintain context integrity.

### 2. Document Processing Pipeline

The document processing flow includes:

1. Loading documents from multiple sources and formats (PDF, text)
2. Applying semantic chunking to break documents into meaningful units
3. Generating embeddings for each chunk
4. Storing embeddings in a vector database (FAISS)
5. Caching the vectorstore for efficiency

```python
def _process_documents(self):
    # ... document loading and processing logic
    splitter = SemanticTextSplitter(chunk_size=700, chunk_overlap=150)
    chunks = splitter.split_documents(all_docs)
    embeddings = OllamaEmbeddings(base_url=self.ollama_server, model=self.embed_model)
    self.vectorstore = FAISS.from_documents(chunks, embeddings)
```

**Technical considerations**:
- Chunk size (700) and overlap (150) balance context preservation with retrieval precision
- Error handling captures and logs issues with specific file types
- Caching reduces startup time for subsequent runs

### 3. Retrieval and Response Generation

The system implements a conversational retrieval chain that:

1. Takes a user query and conversation history
2. Retrieves relevant document chunks (k=12)
3. Passes relevant context to the language model
4. Generates a response grounded in the document content
5. Manages conversation context over multiple turns

```python
self.qa_chain = ConversationalRetrievalChain.from_llm(
    llm=self.llm,
    chain_type="stuff",
    retriever=self.retriever,
    return_source_documents=True
)
```

**Implementation details**:
- Uses the "stuff" chain type to combine retrieved documents
- Maintains a capped conversation history (10 turns)
- Implements streaming for real-time response generation
- Tracks and visualizes source document relevance

### 4. Streaming Implementation

The `StreamHandler` provides real-time output generation:

```python
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.update(value=self.text)
```

This creates a more engaging user experience by showing responses as they're generated rather than waiting for the complete response.

### 5. User Interface

The system offers two interfaces:

1. **Command-Line Interface (CLI)**: Simple text-based interaction
2. **Graphical User Interface (GUI)**: Rich web-based interface built with Gradio

The Gradio interface includes:
- Chat history with styled message bubbles
- Document upload functionality
- Source relevance visualization
- Chat export capabilities
- System information panel

```python
def create_gradio_interface(assistant):
    # ... Gradio UI component definition
```

### 6. System Prompt Engineering

The system uses a carefully crafted prompt to guide the language model's behavior:

```python
SYSTEM_PROMPT = """You are MARBET's AI-powered Event Assistant for the Sales Trip 2024...
# ... detailed instructions about behavior, context, and response style
"""
```

This prompt engineering ensures:
- Responses are grounded in the provided documents
- The assistant maintains appropriate context about the event
- Ambiguous terms (like "Election program") are properly interpreted
- Responses follow a consistent structure and tone

### 7. Source Document Visualization

The system generates visualizations of source document relevance:

```python
def visualize_sources(self, source_documents):
    # ... visualization logic
```

This feature provides transparency into the retrieval process, helping users understand which documents influenced the response.

## Advanced Techniques

### 1. Session Management

The system implements stateful sessions by:
- Generating unique session IDs
- Persisting conversation history
- Enabling export of complete conversations

```python
def _save_session(self):
    session_path = os.path.join(self.cache_dir, "sessions", f"{self.session_id}.pkl")
    # ... session saving logic
```

### 2. Dynamic Document Addition

The system supports adding new documents at runtime:

```python
def add_document(self, file_path):
    # ... document loading and processing logic
```

This allows updating the knowledge base without restarting the application.

### 3. Error Handling and Logging

Comprehensive error handling and logging ensure system stability:

```python
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
```

Each major operation includes try/except blocks with appropriate error logging, enhancing debuggability and reliability.

## Performance Considerations

1. **Vectorstore Caching**: Avoids reprocessing documents by caching the vectorstore
2. **Semantic Chunking**: Improves retrieval quality through meaningful document splitting
3. **Parameterized Retrieval**: The k=12 parameter retrieves sufficient context without overwhelming the model
4. **Streaming**: Reduces perceived latency through progressive response rendering

## Conclusion

The MARBET Event Assistant demonstrates sophisticated implementation of RAG technologies. Its custom semantic text splitting, efficient vector retrieval, and flexible user interfaces create a robust solution for document-grounded conversational AI. The system's architecture balances performance, accuracy, and user experience while providing transparency into its operation through source visualization and comprehensive logging.

The modular design allows for easy extension and customization, while careful error handling ensures reliability in production environments. This implementation represents a mature approach to building domain-specific assistants that leverage both retrieval and generation capabilities of modern AI systems.